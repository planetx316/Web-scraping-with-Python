{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    def _init_(self, site):\n",
    "        self.site=site\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __init__ method uses a website to extract as a parameter. Later you will pass “https://news.google.com/” as a parameter. The Scraper class has a method called scrape that you will call whenever you want to retrieve data from the site you passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(self):\n",
    "        r = urllib.request.urlopen(self.site)\n",
    "        html = r.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The urlopen () function sends a request to a website and returns a Response object in which its HTML code is stored, along with additional data. The response of the function. read () returns the HTML of the Response object. All the HTML for the website is in the html variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def scrape(self):\n",
    "        r = urllib.request.urlopen(self.site)\n",
    "        html = r.read()\n",
    "        parser = \"html.parser\"\n",
    "        sp = BeautifulSoup(html,parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "   def scrape(self):\n",
    "        r = urllib.request.urlopen(self.site)\n",
    "        html = r.read()\n",
    "        parser = \"html.parser\"\n",
    "        sp = BeautifulSoup(html,parser)\n",
    "        for tag in sp.find_all(\"a\"):\n",
    "            url = tag.get(\"href\")\n",
    "            if url is None:\n",
    "                continue\n",
    "            if \"articles\" in url:\n",
    "                print(\"\\n\" + url)\n",
    "                \n",
    "class Scraper:\n",
    "    def __init__(self, site):\n",
    "        self.site = site\n",
    "\n",
    "    def scrape(self):\n",
    "        r = urllib.request.urlopen(self.site)\n",
    "        html = r.read()\n",
    "        parser = \"html.parser\"\n",
    "        sp = BeautifulSoup(html,parser)\n",
    "        for tag in sp.find_all(\"a\"):\n",
    "            url = tag.get(\"href\")\n",
    "            if url is None:\n",
    "                continue\n",
    "            if \"articles\" in url:\n",
    "                print(\"\\n\" + url)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = \"https://news.google.com/\"\n",
    "Scraper(news).scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
